{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 吸收状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from muzero.config import PLANE_NUM, MuZeroConfig\n",
    "from muzero.self_play import SelfPlay\n",
    "from muzero.buffer_utils import make_target\n",
    "import xqcpp\n",
    "import numpy as np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_game_history(gh, title):\n",
    "    # 显示游戏历史信息【政策基于子节点访问次数】\n",
    "    print(f\"{title} {len(gh.root_values)=}\")\n",
    "    for i in range(len(gh.root_values)):\n",
    "        pi = {}\n",
    "        cv = gh.child_visits[i]\n",
    "        for a, prob in enumerate(cv):\n",
    "            if prob > 0.0:\n",
    "                pi[xqcpp.m2a(a)] = round(prob, 4)\n",
    "        print(\n",
    "            \"根值 {:.2f} 政策 {} 合计 {} 下一步移动 {} reward = {:.2f}\".format(\n",
    "                gh.root_values[i],\n",
    "                pi,\n",
    "                round(sum(pi.values()), 4),\n",
    "                xqcpp.m2a(gh.action_history[i + 1]),\n",
    "                gh.reward_history[i + 1],\n",
    "            )\n",
    "        )\n",
    "        print(\"=\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 固定种子后可复现\n",
    "config = MuZeroConfig()\n",
    "config.batch_size = 128\n",
    "config.training_steps = 200\n",
    "init_fen = \"3k5/2P1P4/9/9/9/9/9/9/4p1p2/5K3 r - 100 0 190\"\n",
    "config.num_simulations = 60\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 演示棋局"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymxq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"xqv1\",\n",
    "    init_fen=config.init_fen,\n",
    "    render_mode=\"ansi\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9 \u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[34m将\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\n",
      "8 \u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[31m兵\u001b[0m\u001b[30m＋\u001b[0m\u001b[31m兵\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\n",
      "7 \u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\n",
      "6 \u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\n",
      "5 \u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\n",
      "4 \u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\n",
      "3 \u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\n",
      "2 \u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\n",
      "1 \u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[34m卒\u001b[0m\u001b[30m＋\u001b[0m\u001b[34m卒\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\n",
      "0 \u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[31m帅\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\u001b[30m＋\u001b[0m\n",
      "  ０１２３４５６７８\n",
      "轮到红方走子\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obs, info = env.reset()\n",
    "print(env.render())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修正模拟生成数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from muzero.buffer_utils import Buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = Buffer(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "调试MCST目录/home/ldf/muzero_experiment/runs_000/train_mcts_000\n",
      "\n",
      "rollout_0 len(gh.root_values)=1\n",
      "根值 -0.03 政策 {'2818': 0.15, '2829': 0.1833, '2838': 0.1833, '4838': 0.1833, '4849': 0.15, '4858': 0.15} 合计 0.9999 下一步移动 2838 reward = 1.00\n",
      "==============================\n",
      "调试MCST目录/home/ldf/muzero_experiment/runs_000/train_mcts_001\n",
      "\n",
      "rollout_1 len(gh.root_values)=7\n",
      "根值 -0.01 政策 {'2818': 0.1667, '2829': 0.1667, '2838': 0.1667, '4838': 0.1667, '4849': 0.1667, '4858': 0.1667} 合计 1.0002 下一步移动 2818 reward = 0.00\n",
      "==============================\n",
      "根值 -0.01 政策 {'4131': 0.1333, '4140': 0.2, '4151': 0.1667, '6151': 0.2, '6160': 0.1333, '6171': 0.1667} 合计 1.0 下一步移动 4140 reward = 0.00\n",
      "==============================\n",
      "根值 0.04 政策 {'5040': 1.0} 合计 1.0 下一步移动 5040 reward = 0.00\n",
      "==============================\n",
      "根值 0.02 政策 {'6151': 0.2667, '6160': 0.4667, '6171': 0.2667} 合计 1.0001 下一步移动 6160 reward = 0.00\n",
      "==============================\n",
      "根值 -0.00 政策 {'1808': 0.1333, '1819': 0.1333, '1828': 0.2, '4041': 0.1333, '4838': 0.1333, '4849': 0.1333, '4858': 0.1333} 合计 0.9998 下一步移动 1828 reward = 0.00\n",
      "==============================\n",
      "根值 0.00 政策 {'6050': 0.5, '6070': 0.5} 合计 1.0 下一步移动 6050 reward = 0.00\n",
      "==============================\n",
      "根值 0.04 政策 {'4041': 0.4667, '4050': 0.5333} 合计 1.0 下一步移动 4050 reward = 1.00\n",
      "==============================\n",
      "调试MCST目录/home/ldf/muzero_experiment/runs_000/train_mcts_002\n",
      "\n",
      "rollout_2 len(gh.root_values)=1\n",
      "根值 0.03 政策 {'2818': 0.15, '2829': 0.2333, '2838': 0.1833, '4838': 0.1667, '4849': 0.1167, '4858': 0.15} 合计 1.0 下一步移动 2829 reward = 1.00\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# 现模型\n",
    "ghs = []\n",
    "for i in range(3):\n",
    "    player = SelfPlay(config, i,init_fen)\n",
    "    gh = player.rollout(1)\n",
    "    ghs.append(gh)\n",
    "    view_game_history(gh, f\"rollout_{i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一例\n",
    "case1 = ghs[0]\n",
    "case1.root_values = [0.999]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "moves_policy = {\n",
    "    \"2818\": 0.005,\n",
    "    \"2829\": 0.490,\n",
    "    \"2838\": 0.490,\n",
    "    \"4838\": 0.005,\n",
    "    \"4849\": 0.005,\n",
    "    \"4858\": 0.005,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_policy = [0] * 2086\n",
    "for k, v in moves_policy.items():\n",
    "    actions_policy[xqcpp.m2a(k)] = v\n",
    "case1.child_visits[0] = actions_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case1 len(gh.root_values)=1\n",
      "根值 1.00 政策 {'2818': 0.005, '2829': 0.49, '2838': 0.49, '4838': 0.005, '4849': 0.005, '4858': 0.005} 合计 1.0 下一步移动 2838 reward = 1.00\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# 显示修改后的政策\n",
    "view_game_history(case1, \"case1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2086, 643]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case1.action_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer.save_game(case1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case2 len(gh.root_values)=1\n",
      "根值 1.00 政策 {'2818': 0.005, '2829': 0.49, '2838': 0.49, '4838': 0.005, '4849': 0.005, '4858': 0.005} 合计 1.0 下一步移动 2838 reward = 1.00\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "case2 = case1\n",
    "case2.action_history[1] = xqcpp.m2a(\"2838\")\n",
    "# 显示修改后的政策\n",
    "view_game_history(case2, \"case2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer.save_game(case2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer.save_game(ghs[1])\n",
    "buffer.save_game(ghs[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_batch, (\n",
    "    observation_batch,\n",
    "    action_batch,\n",
    "    value_batch,\n",
    "    reward_batch,\n",
    "    policy_batch,\n",
    "    weight_batch,\n",
    "    gradient_scale_batch,\n",
    ") = buffer.get_batch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = (\n",
    "    observation_batch,\n",
    "    action_batch,\n",
    "    value_batch,\n",
    "    reward_batch,\n",
    "    policy_batch,\n",
    "    weight_batch,\n",
    "    gradient_scale_batch,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解码查看目标移动\n",
    "from muzero.feature_utils import decode_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000\n",
      "2838\n",
      "0000\n",
      "0000\n",
      "0000\n",
      "0000\n"
     ]
    }
   ],
   "source": [
    "# 注意 自然序号 0 -> 0001 只有空白才编码为 0000\n",
    "for a in action_batch[0]:\n",
    "    print(decode_action(a, True))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f60b8fbca90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(config.seed)\n",
    "torch.manual_seed(config.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from muzero.models import MuZeroNetwork\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MuZeroNetwork(config)\n",
    "model.to(\"cuda\")\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=config.lr_init,\n",
    "    momentum=config.momentum,\n",
    "    weight_decay=config.weight_decay,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from muzero.trainer_utils import update_weights, update_lr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方案比较\n",
    "\n",
    "批量设定64，训练200次\n",
    "吸收状态其值及即时奖励全部设置为0，反向传播时由于访问次数增加，导致数值变小。\n",
    "\n",
    "1. `action`以`NUM_ACTIONS`填充,政策为空 值估计 1.07 损失 value:0.00 reward:0.00 policy:0.81\n",
    "2. 维持终止状态`action`及上一`action`设为1，其余为0的政策 值估计 0.97 value:0.00 reward:0.00 policy:0.81\n",
    "\n",
    "训练时间：\n",
    "batch_size = 128 使用 GPU 4.4 G -> 256 ~9 G\n",
    "+ cpu 21m4s\n",
    "+ GPU  1m7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0nth loss [total:13.57 value:1.91 reward:0.61 policy:12.48] lr:0.00200\n",
      "     20nth loss [total:4.44 value:0.37 reward:0.13 policy:4.22] lr:0.00200\n",
      "     40nth loss [total:2.73 value:0.14 reward:0.05 policy:2.64] lr:0.00199\n",
      "     60nth loss [total:2.16 value:0.06 reward:0.03 policy:2.12] lr:0.00199\n",
      "     80nth loss [total:2.02 value:0.04 reward:0.02 policy:1.98] lr:0.00198\n",
      "    100nth loss [total:1.98 value:0.03 reward:0.02 policy:1.94] lr:0.00198\n",
      "    120nth loss [total:1.96 value:0.03 reward:0.02 policy:1.93] lr:0.00197\n",
      "    140nth loss [total:1.95 value:0.03 reward:0.02 policy:1.92] lr:0.00197\n",
      "    160nth loss [total:1.95 value:0.03 reward:0.02 policy:1.92] lr:0.00197\n",
      "    180nth loss [total:1.94 value:0.03 reward:0.02 policy:1.91] lr:0.00196\n"
     ]
    }
   ],
   "source": [
    "for training_step in range(config.training_steps):\n",
    "    (\n",
    "        total_loss,\n",
    "        value_loss,\n",
    "        reward_loss,\n",
    "        policy_loss,\n",
    "    ) = update_weights(batch, model, optimizer, config, False)\n",
    "    update_lr(training_step, optimizer, config)\n",
    "    if training_step % 20 == 0:\n",
    "        print(\n",
    "            \"{:>7d}nth loss [total:{:.2f} value:{:.2f} reward:{:.2f} policy:{:.2f}] lr:{:.5f}\".format(\n",
    "                training_step,\n",
    "                total_loss,\n",
    "                value_loss,\n",
    "                reward_loss,\n",
    "                policy_loss,\n",
    "                optimizer.param_groups[0][\"lr\"],\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储模型\n",
    "torch.save(model.state_dict(), \"model_weights.pth\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from muzero.feature_utils import obs2feature\n",
    "from muzero.mcts import MCTS, render_root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置调试 mcts 搜索树\n",
    "config.debug_mcts = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = obs2feature(obs, info, flatten=False)\n",
    "to_play = info[\"to_play\"]\n",
    "reset = False\n",
    "with torch.no_grad():\n",
    "    legal_actions = info[\"legal_actions\"]\n",
    "    root, mcts_info = MCTS(config).run(\n",
    "        model,\n",
    "        observation,\n",
    "        legal_actions,\n",
    "        to_play,\n",
    "        False,\n",
    "    )\n",
    "    render_root(root, \"test\", \"svg\", \"mcts_tree\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测、调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "调试MCST目录/home/ldf/muzero_experiment/runs_000/train_mcts_000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "player = SelfPlay(config, 1, init_fen)\n",
    "player.model.set_weights(model.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_fen(env, moves):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicate(env, moves, model):\n",
    "    gh = SelfPlay(config, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "调试MCST目录/home/ldf/muzero_experiment/runs_000/train_mcts_000\n",
      "\n",
      "rollout_0 len(gh.root_values)=1\n",
      "根值 1.10 政策 {'2829': 0.4833, '2838': 0.4333, '4858': 0.0833} 合计 0.9999 下一步移动 2829 reward = 1.00\n",
      "==============================\n",
      "调试MCST目录/home/ldf/muzero_experiment/runs_000/train_mcts_001\n",
      "\n",
      "rollout_1 len(gh.root_values)=1\n",
      "根值 1.10 政策 {'2818': 0.0833, '2829': 0.4833, '2838': 0.4333} 合计 0.9999 下一步移动 2829 reward = 1.00\n",
      "==============================\n",
      "调试MCST目录/home/ldf/muzero_experiment/runs_000/train_mcts_002\n",
      "\n",
      "rollout_2 len(gh.root_values)=1\n",
      "根值 1.10 政策 {'2829': 0.5167, '2838': 0.4833} 合计 1.0 下一步移动 2829 reward = 1.00\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "ghs = []\n",
    "for i in range(3):\n",
    "    player = SelfPlay(config, i)\n",
    "    player.model.set_weights(model.get_weights())\n",
    "    gh = player.rollout(1)\n",
    "    ghs.append(gh)\n",
    "    view_game_history(gh, f\"rollout_{i}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
